{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Taller 07\n",
        "\n",
        "Nombre: Rossy Armendariz\n",
        "\n",
        "Contar con la funcion wc las palabras de cada episodio."
      ],
      "metadata": {
        "id": "_46dAm8BTC9G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oVQuLJcPl4L",
        "outputId": "159ce315-e159-4d44-e5bb-15c28a254f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text     wc\n",
            "0  As part of MIT course 6S099, Artificial Genera...  13424\n",
            "1  As part of MIT course 6S099 on artificial gene...  10217\n",
            "2  You've studied the human mind, cognition, lang...   5989\n",
            "3  What difference between biological neural netw...   5993\n",
            "4  The following is a conversation with Vladimir ...   6374\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "file_path = '/content/sample_data/podcastdata_dataset.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "def wc(text):\n",
        "    return len(str(text).split())\n",
        "\n",
        "df['wc'] = df['text'].apply(wc)\n",
        "\n",
        "print(df[['text', 'wc']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se divide las oraciones de cada episodio y se guarda una nueva data."
      ],
      "metadata": {
        "id": "vf_tiwxrTbv9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ryc25U5UPl4N"
      },
      "outputs": [],
      "source": [
        "new_data = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    ep_id = row['id']\n",
        "    text = row['text']\n",
        "\n",
        "    sentences = sent_tokenize(str(text))\n",
        "\n",
        "    for st_id, sentence in enumerate(sentences, start=1):\n",
        "        new_data.append({'ep_id': ep_id, 'st_id': st_id, 'text': sentence, 'wc': len(sentence.split())})\n",
        "\n",
        "df_split_sentences = pd.DataFrame(new_data)\n",
        "\n",
        "df_split_sentences.to_csv('/content/sample_data/podcastdata_split_sentences.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se realiza un embedding con word2vec."
      ],
      "metadata": {
        "id": "XNTxatHJTgFk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anN7Zey-Pl4N",
        "outputId": "dd1a83fe-c2ad-4c30-f6a8-1b97f1b5d0e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ep_id  st_id                                               text  wc  \\\n",
            "0      1      1  As part of MIT course 6S099, Artificial Genera...  19   \n",
            "1      1      2                     He is a professor here at MIT.   7   \n",
            "2      1      3  He's a physicist, spent a large part of his ca...  17   \n",
            "3      1      4  But he's also studied and delved into the bene...  17   \n",
            "4      1      5  Amongst many other things, he is the cofounder...  24   \n",
            "\n",
            "                                           embedding  \n",
            "0  [0.05451241684042745, -0.5021118316799402, 0.3...  \n",
            "1  [0.058552719031771026, 0.19645274678866068, -0...  \n",
            "2  [0.13953624814748763, 0.23896625498309731, 0.2...  \n",
            "3  [0.1628580184735577, 0.13386402366792455, 0.30...  \n",
            "4  [0.038618298976317696, 0.25793242778467096, -0...  \n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Paso 1: Preprocesar las oraciones para entrenar Word2Vec\n",
        "sentences_for_w2v = [simple_preprocess(sentence) for sentence in df_split_sentences['text']]\n",
        "\n",
        "# Paso 2: Entrenar el modelo Word2Vec\n",
        "w2v_model = Word2Vec(sentences=sentences_for_w2v, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Paso 3: Crear embeddings para cada oración\n",
        "def calculate_embedding(sentence):\n",
        "    words = simple_preprocess(sentence)\n",
        "    word_vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    if word_vectors:\n",
        "        # Calcular el promedio manualmente sin NumPy\n",
        "        summed_vector = [sum(x) for x in zip(*word_vectors)]\n",
        "        avg_vector = [val / len(word_vectors) for val in summed_vector]\n",
        "        return avg_vector\n",
        "    else:\n",
        "        return [0] * w2v_model.vector_size  # Vector nulo si no hay palabras conocidas\n",
        "\n",
        "# Aplicar la función a cada oración\n",
        "df_split_sentences['embedding'] = df_split_sentences['text'].apply(calculate_embedding)\n",
        "\n",
        "# Guardar el DataFrame con embeddings\n",
        "df_split_sentences.to_csv('/content/sample_data/podcastdata_split_with_wc_word2vec_no_numpy.csv', index=False)\n",
        "\n",
        "# Verificar las primeras filas\n",
        "print(df_split_sentences.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se agrupa los embeddings para tener dividido en cluster de topics."
      ],
      "metadata": {
        "id": "-BEj22f-Tkn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar el archivo con los embeddings\n",
        "file_path = '/content/sample_data/podcastdata_split_with_wc_word2vec_no_numpy.csv'\n",
        "df_split_sentences = pd.read_csv(file_path)\n",
        "\n",
        "# Convertir la columna de embeddings a listas de floats\n",
        "import ast  # Necesario para convertir strings en listas\n",
        "df_split_sentences['embedding'] = df_split_sentences['embedding'].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "# Crear una matriz con los embeddings\n",
        "embedding_matrix = df_split_sentences['embedding'].tolist()\n",
        "\n",
        "# Calcular la matriz de similitud coseno\n",
        "similarity_matrix = cosine_similarity(embedding_matrix)\n",
        "\n",
        "# Clustering (K-Means)\n",
        "num_clusters = 5  # Cambia este valor según los tópicos que quieras encontrar\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "df_split_sentences['cluster'] = kmeans.fit_predict(embedding_matrix)\n",
        "\n",
        "# Guardar los resultados\n",
        "df_split_sentences.to_csv('/content/sample_data/podcastdata_with_clusters.csv', index=False)\n",
        "\n",
        "# Visualizar las primeras filas del DataFrame\n",
        "print(df_split_sentences.head())\n"
      ],
      "metadata": {
        "id": "AmYPm2ZERwfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lIbn-7YJTBQr"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}