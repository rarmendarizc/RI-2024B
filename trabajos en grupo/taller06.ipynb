{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Taller 06: Bases de datos Vectoriales**\n",
    "\n",
    "Nombres: Rossy Armendariz, Alejandro Chavez, Wilmer Rivas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Recuperación con TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Title  \\\n",
      "0            Kansas Saloon Smashers   \n",
      "1     Love by the Light of the Moon   \n",
      "2           The Martyred Presidents   \n",
      "3  Terrible Teddy, the Grizzly King   \n",
      "4            Jack and the Beanstalk   \n",
      "\n",
      "                                                Plot  \n",
      "0  A bartender is working at a saloon, serving dr...  \n",
      "1  The moon, painted with a smiling face hangs ov...  \n",
      "2  The film, just over a minute long, is composed...  \n",
      "3  Lasting just 61 seconds and consisting of two ...  \n",
      "4  The earliest known adaptation of the classic f...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Leer el archivo CSV que contiene los datos de las películas\n",
    "data = pd.read_csv('./data/wiki_movie_plots_deduped.csv')\n",
    "\n",
    "# Filtrar solo las columnas \"Title\" y \"Plot\" necesarias para el análisis\n",
    "selected_data = data[['Title', 'Plot']]\n",
    "\n",
    "# Visualizar las primeras filas del DataFrame resultante\n",
    "print(selected_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Title  \\\n",
      "34283  Fantastic Journey to OZ   \n",
      "432              Grandma's Boy   \n",
      "28716      72 Mile - Ek Pravas   \n",
      "28379         Once Upon a Time   \n",
      "34034         Once Upon A Time   \n",
      "\n",
      "                                                    Plot     Score  \n",
      "34283  The envious and power-hungry Urfin Jus wants t...  0.235492  \n",
      "432    The grandma's boy is a timid coward who cannot...  0.232582  \n",
      "28716  A young boy decides to escape from his boardin...  0.225330  \n",
      "28379  A sage gives Rahul a book containing various c...  0.220203  \n",
      "34034  A sage gives Rahul a book containing various c...  0.220203  \n"
     ]
    }
   ],
   "source": [
    "# Configuración del vectorizador TF-IDF para procesar el texto\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Generación de la matriz TF-IDF basada en la columna 'Plot'\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(selected_data['Plot'])\n",
    "\n",
    "# Definición de la función para ejecutar búsquedas basadas en TF-IDF\n",
    "def tfidf_search(query_text, results_count=5):\n",
    "  \n",
    "    # Transformar la consulta en vector TF-IDF\n",
    "    query_vector = tfidf_vectorizer.transform([query_text])\n",
    "    \n",
    "    # Calcular la similitud entre la consulta y los documentos\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Seleccionar los índices de los documentos más relevantes\n",
    "    best_match_indices = similarity_scores.argsort()[-results_count:][::-1]\n",
    "    \n",
    "    # Crear un DataFrame con los resultados\n",
    "    top_results = selected_data.iloc[best_match_indices][['Title', 'Plot']].copy()\n",
    "    top_results['Score'] = similarity_scores[best_match_indices]\n",
    "    \n",
    "    return top_results\n",
    "\n",
    "# Ejemplo de consulta\n",
    "search_query = \"A young boy discovers magic\"\n",
    "search_results = tfidf_search(search_query)\n",
    "\n",
    "# Mostrar los resultados de la búsqueda\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de resultados.\n",
    "Los documentos recuperados contienen términos relacionados con young boy o temas similares como \"boy\", \"timid\", \"escape\", pero no necesariamente magic, lo que indica que TF-IDF priorizó coincidencias textuales exactas y no relaciones semántica para identificar términos relacionados o sinónimos. Además se visualiza que no existe una grnade diferencia entre las similitudes de los documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Recuperación con BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice 'movie_plots' eliminado correctamente.\n",
      "Índice 'movie_plots' creado con éxito.\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "import pandas as pd\n",
    "\n",
    "# Establecer conexión con el servidor Elasticsearch\n",
    "elastic_client = Elasticsearch(\"http://localhost:9200\")  # Incluyendo el esquema http\n",
    "\n",
    "# Nombre del índice a manejar\n",
    "index_name = \"movie_plots\"\n",
    "\n",
    "# Verificar si el índice ya existe, y eliminarlo si es necesario\n",
    "if elastic_client.indices.exists(index=index_name):\n",
    "    elastic_client.indices.delete(index=index_name)\n",
    "    print(f\"Índice '{index_name}' eliminado correctamente.\")\n",
    "\n",
    "# Crear un índice nuevo en Elasticsearch\n",
    "elastic_client.indices.create(index=index_name)\n",
    "print(f\"Índice '{index_name}' creado con éxito.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso de indexación finalizado.\n"
     ]
    }
   ],
   "source": [
    "# Función para cargar los datos en el índice existente\n",
    "def cargar_datos():\n",
    "    for fila, datos in selected_data.iterrows():  # Usamos el DataFrame filtrado\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": {\n",
    "                \"Indice\": int(fila),  # Almacena el índice original convertido a un entero\n",
    "                \"Titulo\": datos['Title'],\n",
    "                \"Sinopsis\": datos['Plot']\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Enviar los documentos al índice\n",
    "bulk(elastic_client, cargar_datos())\n",
    "print(\"Proceso de indexación finalizado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Title  \\\n",
      "Index                                     \n",
      "12820         Sabrina the Teenage Witch   \n",
      "24430                             Devta   \n",
      "432                       Grandma's Boy   \n",
      "32935  Magic Serpent !The Magic Serpent   \n",
      "33953       Mary and the Witch's Flower   \n",
      "\n",
      "                                                    Plot      Score  \n",
      "Index                                                                \n",
      "12820  The movie centers around Sabrina Sawyer, who i...  12.786088  \n",
      "24430  This film narrated the story of a king who los...  11.225356  \n",
      "432    The grandma's boy is a timid coward who cannot...  10.938396  \n",
      "32935  The Oumi Kingdom, ruled peacefully by Lord Oga...  10.933238  \n",
      "33953  A fire burns as workers struggle to control it...  10.827491  \n"
     ]
    }
   ],
   "source": [
    "# Función para realizar consultas con BM25\n",
    "def search_bm25(query, top_k=5):\n",
    "    # Construir el cuerpo de la consulta\n",
    "    query_body = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"Sinopsis\": query  # Cambiar \"Plot\" por \"Sinopsis\" si así se indexaron los datos\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k  # Limitar el número de resultados\n",
    "    }\n",
    "\n",
    "    # Ejecutar la consulta en Elasticsearch\n",
    "    response = elastic_client.search(index=index_name, body=query_body)\n",
    "\n",
    "    # Crear una lista para almacenar los resultados\n",
    "    results = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        # Asegurarse de manejar el caso en que 'Indice' no exista\n",
    "        results.append({\n",
    "            \"Index\": hit['_source'].get('Indice', None),  # Usar .get() para evitar errores\n",
    "            \"Title\": hit['_source'].get('Titulo', \"Unknown Title\"),\n",
    "            \"Plot\": hit['_source'].get('Sinopsis', \"No Plot Available\"),\n",
    "            \"Score\": hit['_score']  # Puntuación de relevancia\n",
    "        })\n",
    "\n",
    "    # Convertir la lista en un DataFrame\n",
    "    # Usar .set_index solo si \"Index\" tiene valores válidos\n",
    "    df_results = pd.DataFrame(results)\n",
    "    if 'Index' in df_results.columns and df_results['Index'].notnull().all():\n",
    "        df_results = df_results.set_index(\"Index\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# Ejemplo de ejecución\n",
    "query = \"A young boy discovers magic\"  # Texto de búsqueda\n",
    "results = search_bm25(query)  # Recuperar los documentos relevantes\n",
    "print(results)  # Mostrar los resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de resultados.\n",
    "Se puede observar que BM25 proporciona resultados que se alinean mejor con el tema de la consulta (\"A young boy discovers magic\"). Por ejemplo, títulos como Sabrina the Teenage Witch y Mary and the Witch's Flower tienen asociaciones claras con magia y juventud, lo cual está más alineado con la intención de la consulta. Mientras que TF-IDF, algunos casos incluye resultados como 72 Mile - Ek Pravas, que no parecen tan relacionados con magia, lo que puede deberse a su dependencia en la frecuencia relativa de términos sin considerar contexto. Por otro lado, los puntajes de BM25 son más variables (12.78 para el documento más relevante frente a ~10.8 para otros), lo que indica una priorización más clara de los documentos más relevantes.\n",
    "En TF-IDF, los puntajes son más uniformes (~0.22-0.23), lo que sugiere que su capacidad para distinguir relevancia es más limitada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Recuperación con FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.48.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.27.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\wbrja\\appdata\\roaming\\python\\python310\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wbrja\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wbrja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Para mostrar barra de progreso\n",
    "import os\n",
    "import faiss\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando vectores de embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando lotes: 100%|██████████| 546/546 [36:59<00:00,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del arreglo de embeddings: (34886, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Para mostrar una barra de progreso\n",
    "\n",
    "# Modelo preentrenado para generar embeddings\n",
    "modelo_embeddings = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Configuración del tamaño de lotes\n",
    "tamano_lote = 64\n",
    "\n",
    "# Extraer la columna \"Plot\" del DataFrame 'selected_data' como lista\n",
    "sinopsis = selected_data['Plot'].tolist()\n",
    "\n",
    "# Inicializar una lista vacía para almacenar los embeddings generados\n",
    "vectores_embeddings = []\n",
    "\n",
    "print(\"Generando vectores de embeddings...\")\n",
    "\n",
    "# Procesar las sinopsis en lotes y generar los vectores\n",
    "for inicio in tqdm(range(0, len(sinopsis), tamano_lote), desc=\"Procesando lotes\"):\n",
    "    lote = sinopsis[inicio:inicio + tamano_lote]  # Crear un lote a partir de la lista\n",
    "    lote_embeddings = modelo_embeddings.encode(lote)  # Generar embeddings para el lote\n",
    "    vectores_embeddings.extend(lote_embeddings)  # Agregar los embeddings al resultado final\n",
    "\n",
    "# Convertir los vectores de embeddings en un arreglo NumPy\n",
    "vectores_embeddings = np.array(vectores_embeddings)\n",
    "\n",
    "print(f\"Dimensiones del arreglo de embeddings: {vectores_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de vectores en el índice FAISS: 34886\n"
     ]
    }
   ],
   "source": [
    "# Crear índice FAISS para búsqueda basada en vectores\n",
    "dimension_vectores = vectores_embeddings.shape[1]  # Dimensión de cada vector de embedding\n",
    "indice_faiss = faiss.IndexFlatL2(dimension_vectores)  # Crear índice usando la métrica de distancia Euclidiana (L2)\n",
    "\n",
    "# Agregar los embeddings al índice FAISS\n",
    "indice_faiss.add(vectores_embeddings)  # Cargar los vectores en el índice\n",
    "\n",
    "# Verificar el total de vectores almacenados en el índice\n",
    "print(f\"Total de vectores en el índice FAISS: {indice_faiss.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Titulo  \\\n",
      "13065          The Midas Touch   \n",
      "17232                  Sleight   \n",
      "21101           The Mirror Boy   \n",
      "7329   The Boy and the Pirates   \n",
      "28728            Dusari Goshta   \n",
      "\n",
      "                                                Sinopsis  Distancia  \n",
      "13065  Drama about a 12-year-old boy who fantasises a...   0.965923  \n",
      "17232  A young street magician named Bo (Jacob Latimo...   1.077003  \n",
      "21101  The film tells the uplifting story of a young ...   1.107963  \n",
      "7329   A boy, Jimmy Warren, living along the coast in...   1.161079  \n",
      "28728  A young boy from the lower caste resorts to pe...   1.170947  \n"
     ]
    }
   ],
   "source": [
    "# Función para realizar búsquedas con FAISS basada en embeddings\n",
    "def realizar_busqueda_faiss(consulta, num_resultados=5):\n",
    "    # Generar el embedding para la consulta\n",
    "    vector_consulta = modelo_embeddings.encode([consulta])\n",
    "\n",
    "    # Consultar el índice FAISS para obtener los vecinos más cercanos\n",
    "    distancias, indices_vecinos = indice_faiss.search(vector_consulta, num_resultados)\n",
    "\n",
    "    # Extraer los datos relevantes del DataFrame original\n",
    "    resultados = selected_data.iloc[indices_vecinos.flatten()][['Title', 'Plot']].copy()\n",
    "    \n",
    "    # Ajustar nombres de columnas y agregar la distancia calculada\n",
    "    resultados.rename(columns={'Title': 'Titulo', 'Plot': 'Sinopsis'}, inplace=True)\n",
    "    resultados['Distancia'] = distancias.flatten()\n",
    "\n",
    "    return resultados\n",
    "\n",
    "# Ejemplo práctico de uso\n",
    "consulta_prueba = \"A young boy discovers magic\"\n",
    "resultados_prueba = realizar_busqueda_faiss(consulta_prueba)\n",
    "\n",
    "# Imprimir los resultados obtenidos\n",
    "print(resultados_prueba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de resultados\n",
    "FAISS nos da documentos que tienen una relación más semántica con la consulta \"A young boy discovers magic\". Por ejemplo, The Midas Touch y Sleight son altamente relevantes, debido a que combinan temas de magia y juventud. Existe una mejor comprensión mejor al enfocarse en el contexto general y no solamente en coincidencias exactas. Las distancias entre el embedding de la consulta y los documentos son claras (0.96 a 1.17). Esto indica un orden jerárquico efectivo en la similitud semántica.\n",
    "Sin embargo, FAISS presenta un resultado superior al identificar relaciones semánticas más profundas. Los resultados de BM25 están más relacionados con palabras clave exactas, mientras que FAISS entiende el concepto general. BM25 mejora sobre TF-IDF al ajustar la relevancia considerando la longitud de los documentos y la saturación de términos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Recuperación con ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuración de ChromaDB y embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando el dispositivo: cuda\n",
      "Lote 1 procesado exitosamente.\n",
      "Lote 2 procesado exitosamente.\n",
      "Lote 3 procesado exitosamente.\n",
      "Lote 4 procesado exitosamente.\n",
      "Lote 5 procesado exitosamente.\n",
      "Lote 6 procesado exitosamente.\n",
      "Lote 7 procesado exitosamente.\n",
      "Se han agregado 34886 documentos y generado sus embeddings.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import torch\n",
    "\n",
    "# Verificar si la GPU está disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Usando el dispositivo: {device}\")\n",
    "\n",
    "# Cargar el archivo CSV de películas\n",
    "movies_df = pd.read_csv(\"./wiki_movie_plots_deduped.csv\")\n",
    "\n",
    "# Filtrar campos necesarios\n",
    "movies_df = movies_df[movies_df['Plot'].notna()]  # Eliminar filas con Plot vacío\n",
    "documents = movies_df['Plot'].tolist()  # Lista de las tramas\n",
    "titles = movies_df['Title'].tolist()   # Lista de los títulos\n",
    "\n",
    "# Configurar cliente Chroma\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Crear la función de embeddings con GPU\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Modelo eficiente y rápido\n",
    "    device=device  # Especificar el dispositivo (GPU o CPU)\n",
    ")\n",
    "\n",
    "# Crear colección con la función de embeddings configurada\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"movies_collection\",\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# Generar IDs únicos para cada documento\n",
    "ids = [f\"movie_{i}\" for i in range(len(movies_df))]\n",
    "\n",
    "# Dividir los datos en lotes\n",
    "batch_size = 5000  # Tamaño del lote (ajustar según sea necesario)\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch_documents = documents[i:i+batch_size]\n",
    "    batch_titles = titles[i:i+batch_size]\n",
    "    batch_ids = ids[i:i+batch_size]\n",
    "    \n",
    "    # Agregar el lote a la colección\n",
    "    collection.add(\n",
    "        documents=batch_documents,  # Tramas del lote\n",
    "        metadatas=[{\"title\": title} for title in batch_titles],  # Metadata del lote\n",
    "        ids=batch_ids  # IDs únicos del lote\n",
    "    )\n",
    "    print(f\"Lote {i // batch_size + 1} procesado exitosamente.\")\n",
    "\n",
    "print(f\"Se han agregado {len(documents)} documentos y generado sus embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta aplicando ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado 1:\n",
      "Trama: An investigative thriller based on the search for a missing youngster.\n",
      "--------------------------------------------------\n",
      "Resultado 2:\n",
      "Trama: A young boy from the lower caste resorts to petty thefts to make both ends meet after he lost his father at the early age. Once the boy realizes the importance of an education, he begins to improve his life and never looks back. Through diligence and dedication, he climbs the social and political ladder to success.\n",
      "--------------------------------------------------\n",
      "Resultado 3:\n",
      "Trama: Thirteen-year-old Jesse is assigned a school project. A photographic self-portrait intended to portray one’s self without resorting to literal representation. Jesse lives with his parents, Sabi and Tim, in the lefty, middle class Toronto neighbourhood of Riverdale. A quiet and distant only-child with budding artistic aspirations, Jesse is inspired by the assignment to look for excitement and meaning in the world around him. Wielding a newly acquired camera, Jesse sets out to capture his surroundings, but soon realizes the undramatic nature of his family, neighbourhood and existence.\n",
      "Meanwhile, Sabi and Tim find themselves questioning Jesse’s developing character as they watch him abandon his childhood personality and mature into an uncommunicative adolescent. Frustrated by his lack of inspiration, Jesse discovers a book in the school library which advises him: “You can never be a real artist until you have made love to a woman.” Taking the text at face value and with the encouragement of a family friend, Jesse begins to look for incident, both foreign and adult, which leads him to an encounter with his young, female neighbour, Amy. Peering into her window at night, he snaps a quick photograph of her.\n",
      "Days later, the two are inadvertently reunited, allowing Jesse an opportunity to explore his prepubescent fascination with the opposite sex. After a long evening of games, exploration and hypnosis, Jesse awakes with confusion and guilt, unsure whether or not he may have overstepped his boundaries. The question of rape consumes his thoughts and Jesse is left struggling to reconcile his uneasy mind.\n",
      "--------------------------------------------------\n",
      "Resultado 4:\n",
      "Trama: A young scientist search for the mysterious events behind the strange deaths of cattles and bright lights in sky. He sets on a journey to find the truth and fears of something more frightening to come.\n",
      "--------------------------------------------------\n",
      "Resultado 5:\n",
      "Trama: Drama about a 12-year-old boy who fantasises about having enough money to be able to cure his grandmother's serious heart condition. When he finds himself in a haunted house, the mysterious owner 'grants' him one wish - the Midas touch. The boy soon learns that it is more of a curse than a blessing when everything he touches turns into gold.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Definir una consulta (puede ser una trama de película o cualquier texto)\n",
    "query = \"A young boy discovers \"\n",
    "\n",
    "# Realizar la consulta en la colección\n",
    "results = collection.query(\n",
    "    query_texts=[query],  # Pasar la consulta como texto\n",
    "    n_results=5  # Número de resultados a devolver\n",
    ")\n",
    "\n",
    "# Mostrar los resultados\n",
    "for i, document in enumerate(results['documents'][0]):  # Accedemos a la lista de documentos en el primer índice\n",
    "    print(f\"Resultado {i + 1}:\")\n",
    "    print(f\"Trama: {document}\")  # Solo mostramos la trama si no hay metadata disponible\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de los resultados\n",
    "\n",
    "Los resultados de ChromaDB, TF-IDF, BM25, y FAISS muestran diferencias notables en cómo recuperan documentos. ChromaDB brinda tramas que combinan elementos de \"magia\" y \"un niño joven\", mostrando relaciones semánticas más profundas con la consulta. Por ejemplo, incluye temas como búsqueda científica o aventuras de jóvenes con trasfondo mágico. FAISS, al igual que ChromaDB, prioriza la semántica, pero sus resultados tienen una ligera variación al incluir documentos con contextos más amplios relacionados con la consulta. BM25 se centra más en términos exactos como \"magic\" y \"boy\", recuperando documentos altamente relevantes, aunque no tan ricos en contexto como los de ChromaDB. Por último, TF-IDF, aunque eficiente, es menos preciso al depender de la frecuencia de las palabras, incluyendo tramas menos relacionadas con \"magia\". Por tanto, ChromaDB y FAISS son más efectivos para consultas complejas y temáticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: Comparación de Resultados\n",
    "\n",
    "1. Relevancia:\n",
    "\n",
    "TF-IDF: Identificó documentos relacionados principalmente con palabras clave. Aunque incluyó títulos interesantes como \"Fantastic Journey to OZ\", algunos resultados como \"72 Mile - Ek Pravas\" fueron menos relevantes, evidenciando la falta de contexto semántico.\n",
    "\n",
    "BM25: Mejoró los resultados al priorizar documentos con mayor densidad y ajuste por longitud, recuperando títulos más relacionados como \"Sabrina the Teenage Witch\". Pero sigue siendo dependiente de palabras clave, y además mostró mejor precisión que TF-IDF.\n",
    "\n",
    "FAISS: Al utilizar embeddings, capturó relaciones semánticas más profundas. Los resultados como \"The Midas Touch\" y \"Sleight\" tuvieron una conexión mejor a la idea de \"magia\" y \"niño joven\". Sin embargo, requiere más recursos computacionales.\n",
    "\n",
    "ChromaDB: Fue el enfoque más completo, combinando semántica con resultados organizados. Recuperó documentos que incluyen magia y temas científicos, mostrando una fuerte conexión temática con la query.\n",
    "\n",
    "2. Ventajas y limitaciones:\n",
    "\n",
    "TF-IDF: Rápido y sencillo, pero limitado en consultas complejas debido a su falta de semántica.\n",
    "BM25: Equilibrado y efectivo para búsquedas basadas en texto, pero no captura relaciones abstractas entre términos.\n",
    "FAISS: Excelente en semántica, identificando conceptos más allá de coincidencias textuales, pero requiere mayor capacidad de procesamiento.\n",
    "ChromaDB: Combina almacenamiento persistente y semántica, increible para grandes volúmenes de datos, aunque necesita configuración más avanzada.\n",
    "\n",
    "En conclusión, ChromaDB y FAISS son ideales para consultas complejas y relaciones semánticas, mientras que BM25 y TF-IDF son adecuados para consultas más directas. La elección dependerá del contexto y los recursos disponibles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
